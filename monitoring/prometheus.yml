
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'ringrift-p2p-monitor'
    leader_node: 'local-mac'

rule_files:
  # - /etc/prometheus/alerting-rules.yaml

scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
        labels:
          instance: 'leader'

  # ==========================================================================
  # P2P orchestrator on all nodes (Feb 2026: Expanded to full cluster)
  # ==========================================================================
  - job_name: 'ringrift-p2p'
    static_configs:
      # Local coordinators
      - targets: ['localhost:8770']
        labels:
          node: "local-mac"
          role: "leader"
          provider: "local"
      - targets: ['100.107.168.125:8770']
        labels:
          node: "mac-studio"
          role: "coordinator"
          provider: "local"

      # Lambda GH200 nodes (11 total)
      - targets: ['100.71.89.91:8770']
        labels:
          node: "lambda-gh200-1"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.110.143.119:8770']
        labels:
          node: "lambda-gh200-2"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.69.101.108:8770']
        labels:
          node: "lambda-gh200-3"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.77.186.124:8770']
        labels:
          node: "lambda-gh200-4"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.83.177.16:8770']
        labels:
          node: "lambda-gh200-5"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.68.208.43:8770']
        labels:
          node: "lambda-gh200-training"
          role: "gpu_training_primary"
          provider: "lambda"
      - targets: ['100.121.230.110:8770']
        labels:
          node: "lambda-gh200-8"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.127.168.116:8770']
        labels:
          node: "lambda-gh200-9"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.100.19.96:8770']
        labels:
          node: "lambda-gh200-10"
          role: "gpu_training_primary"
          provider: "lambda"
      - targets: ['100.106.87.89:8770']
        labels:
          node: "lambda-gh200-11"
          role: "gpu_training_selfplay"
          provider: "lambda"

      # Vultr nodes
      - targets: ['100.94.201.92:8770']
        labels:
          node: "vultr-a100-20gb"
          role: "gpu_selfplay"
          provider: "vultr"
      - targets: ['140.82.15.69:8770']
        labels:
          node: "vultr-a100-20gb-2"
          role: "gpu_selfplay"
          provider: "vultr"

      # Nebius nodes
      - targets: ['100.110.28.41:8770']
        labels:
          node: "nebius-backbone-1"
          role: "backbone"
          provider: "nebius"
      - targets: ['100.106.19.6:8770']
        labels:
          node: "nebius-h100-1"
          role: "gpu_training_selfplay"
          provider: "nebius"
      - targets: ['100.109.195.71:8770']
        labels:
          node: "nebius-h100-3"
          role: "gpu_training_selfplay"
          provider: "nebius"

      # Hetzner CPU nodes (P2P voters)
      - targets: ['100.94.174.19:8770']
        labels:
          node: "hetzner-cpu1"
          role: "cpu_selfplay"
          provider: "hetzner"
      - targets: ['100.67.131.72:8770']
        labels:
          node: "hetzner-cpu2"
          role: "cpu_selfplay"
          provider: "hetzner"
      - targets: ['100.126.21.102:8770']
        labels:
          node: "hetzner-cpu3"
          role: "cpu_selfplay"
          provider: "hetzner"

      # Vast.ai nodes (with Tailscale IPs)
      - targets: ['100.77.85.19:8770']
        labels:
          node: "vast-29118471"
          role: "gpu_selfplay"
          provider: "vast"
      - targets: ['100.76.233.18:8770']
        labels:
          node: "vast-29126088"
          role: "gpu_selfplay"
          provider: "vast"

  # ==========================================================================
  # Node exporter on all nodes (Feb 2026: Expanded to full cluster)
  # ==========================================================================
  - job_name: 'node_exporter'
    static_configs:
      # Local coordinators
      - targets: ['localhost:9100']
        labels:
          node: "local-mac"
          role: "leader"
          provider: "local"
      - targets: ['100.107.168.125:9100']
        labels:
          node: "mac-studio"
          role: "coordinator"
          provider: "local"

      # Lambda GH200 nodes
      - targets: ['100.71.89.91:9100']
        labels:
          node: "lambda-gh200-1"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.110.143.119:9100']
        labels:
          node: "lambda-gh200-2"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.69.101.108:9100']
        labels:
          node: "lambda-gh200-3"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.77.186.124:9100']
        labels:
          node: "lambda-gh200-4"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.83.177.16:9100']
        labels:
          node: "lambda-gh200-5"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.68.208.43:9100']
        labels:
          node: "lambda-gh200-training"
          role: "gpu_training_primary"
          provider: "lambda"
      - targets: ['100.121.230.110:9100']
        labels:
          node: "lambda-gh200-8"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.127.168.116:9100']
        labels:
          node: "lambda-gh200-9"
          role: "gpu_training_selfplay"
          provider: "lambda"
      - targets: ['100.100.19.96:9100']
        labels:
          node: "lambda-gh200-10"
          role: "gpu_training_primary"
          provider: "lambda"
      - targets: ['100.106.87.89:9100']
        labels:
          node: "lambda-gh200-11"
          role: "gpu_training_selfplay"
          provider: "lambda"

      # Vultr nodes
      - targets: ['100.94.201.92:9100']
        labels:
          node: "vultr-a100-20gb"
          role: "gpu_selfplay"
          provider: "vultr"
      - targets: ['140.82.15.69:9100']
        labels:
          node: "vultr-a100-20gb-2"
          role: "gpu_selfplay"
          provider: "vultr"

      # Nebius nodes
      - targets: ['100.110.28.41:9100']
        labels:
          node: "nebius-backbone-1"
          role: "backbone"
          provider: "nebius"
      - targets: ['100.106.19.6:9100']
        labels:
          node: "nebius-h100-1"
          role: "gpu_training_selfplay"
          provider: "nebius"
      - targets: ['100.109.195.71:9100']
        labels:
          node: "nebius-h100-3"
          role: "gpu_training_selfplay"
          provider: "nebius"

      # Hetzner CPU nodes
      - targets: ['100.94.174.19:9100']
        labels:
          node: "hetzner-cpu1"
          role: "cpu_selfplay"
          provider: "hetzner"
      - targets: ['100.67.131.72:9100']
        labels:
          node: "hetzner-cpu2"
          role: "cpu_selfplay"
          provider: "hetzner"
      - targets: ['100.126.21.102:9100']
        labels:
          node: "hetzner-cpu3"
          role: "cpu_selfplay"
          provider: "hetzner"

      # Vast.ai nodes
      - targets: ['100.77.85.19:9100']
        labels:
          node: "vast-29118471"
          role: "gpu_selfplay"
          provider: "vast"
      - targets: ['100.76.233.18:9100']
        labels:
          node: "vast-29126088"
          role: "gpu_selfplay"
          provider: "vast"

  # Unified AI loop (leader only by default)
  - job_name: 'ringrift-unified-loop'
    static_configs:
      - targets: ['localhost:9091']
        labels:
          instance: 'leader'
    scrape_interval: 15s

  # Elo metrics exporter (leader only)
  - job_name: 'ringrift-elo-metrics'
    static_configs:
      - targets: ['localhost:9092']
        labels:
          instance: 'leader'
    scrape_interval: 30s

  # Data quality monitor (leader only)
  - job_name: 'ringrift-data-quality'
    static_configs:
      - targets: ['localhost:9093']
        labels:
          instance: 'leader'
    scrape_interval: 60s

  # ==========================================================================
  # Prometheus Federation - scrape metrics from other Prometheus instances
  # This enables distributed monitoring where the leader aggregates metrics
  # from Prometheus instances running on other nodes (backup monitoring hosts)
  # ==========================================================================
  - job_name: 'prometheus-federation'
    honor_labels: true  # Preserve original labels from federated instances
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job=~"ringrift.*"}'  # All RingRift metrics
        - '{job="node_exporter"}'  # Node hardware metrics
        - '{__name__=~".*_total|.*_seconds|.*_bytes"}'  # Standard metrics
    static_configs:
      - targets: ['100.68.208.43:9090']
        labels:
          source_node: "lambda-gh200-training"
      - targets: ['100.100.19.96:9090']
        labels:
          source_node: "lambda-gh200-10"
    scrape_interval: 30s
    scrape_timeout: 25s
