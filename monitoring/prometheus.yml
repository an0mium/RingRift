
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'ringrift-p2p-monitor'
    leader_node: 'local-mac'

rule_files:
  - alerting-rules.yaml

scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
        labels:
          instance: 'leader'

  # ==========================================================================
  # P2P Orchestrator Metrics - All active cluster nodes
  # Port 8770: P2P HTTP API, Port 9091: Prometheus metrics exporter
  # Generated from distributed_hosts.yaml - Jan 30, 2026
  # ==========================================================================
  - job_name: 'ringrift-p2p'
    scrape_timeout: 10s
    static_configs:
      # Local coordinators
      - targets: ['localhost:8770']
        labels:
          node: "local-mac"
          role: "coordinator"
          provider: "local"
      - targets: ['100.107.168.125:8770']
        labels:
          node: "mac-studio"
          role: "coordinator"
          provider: "local"

      # Lambda GH200 nodes (11 nodes, 96GB each)
      - targets: ['100.71.89.91:8770']
        labels:
          node: "lambda-gh200-1"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.110.143.119:8770']
        labels:
          node: "lambda-gh200-2"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.69.101.108:8770']
        labels:
          node: "lambda-gh200-3"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.77.186.124:8770']
        labels:
          node: "lambda-gh200-4"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.83.177.16:8770']
        labels:
          node: "lambda-gh200-5"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.68.208.43:8770']
        labels:
          node: "lambda-gh200-training"
          role: "gpu_training_primary"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.121.230.110:8770']
        labels:
          node: "lambda-gh200-8"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.127.168.116:8770']
        labels:
          node: "lambda-gh200-9"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.100.19.96:8770']
        labels:
          node: "lambda-gh200-10"
          role: "gpu_training_primary"
          provider: "lambda"
          gpu: "GH200"
      - targets: ['100.106.87.89:8770']
        labels:
          node: "lambda-gh200-11"
          role: "gpu_training_selfplay"
          provider: "lambda"
          gpu: "GH200"

      # Nebius nodes (3 nodes: L40S backbone + 2x H100)
      - targets: ['100.110.28.41:8770']
        labels:
          node: "nebius-backbone-1"
          role: "backbone"
          provider: "nebius"
          gpu: "L40S"
      - targets: ['100.106.19.6:8770']
        labels:
          node: "nebius-h100-1"
          role: "gpu_training_selfplay"
          provider: "nebius"
          gpu: "H100"
      - targets: ['100.109.195.71:8770']
        labels:
          node: "nebius-h100-3"
          role: "gpu_training_selfplay"
          provider: "nebius"
          gpu: "H100"

      # Vultr nodes (2 nodes: A100 vGPU)
      - targets: ['100.94.201.92:8770']
        labels:
          node: "vultr-a100-20gb"
          role: "gpu_selfplay"
          provider: "vultr"
          gpu: "A100-20C"

      # Hetzner CPU nodes (3 nodes: P2P voters, relay-capable)
      - targets: ['100.94.174.19:8770']
        labels:
          node: "hetzner-cpu1"
          role: "cpu_selfplay"
          provider: "hetzner"
          gpu: "none"
      - targets: ['100.67.131.72:8770']
        labels:
          node: "hetzner-cpu2"
          role: "cpu_selfplay"
          provider: "hetzner"
          gpu: "none"
      - targets: ['100.126.21.102:8770']
        labels:
          node: "hetzner-cpu3"
          role: "cpu_selfplay"
          provider: "hetzner"
          gpu: "none"

      # Vast.ai nodes (ephemeral, NAT-blocked - may be unreachable via Tailscale)
      # These are scraped via relay or direct SSH if available
      - targets: ['100.77.85.19:8770']
        labels:
          node: "vast-29118471"
          role: "gpu_selfplay"
          provider: "vast"
          gpu: "RTX3090x8"
      - targets: ['100.76.233.18:8770']
        labels:
          node: "vast-29126088"
          role: "gpu_selfplay"
          provider: "vast"
          gpu: "RTX4060Ti"

  # ==========================================================================
  # Node Exporter - System metrics (CPU, memory, disk, network)
  # ==========================================================================
  - job_name: 'node_exporter'
    scrape_timeout: 10s
    static_configs:
      # Local
      - targets: ['localhost:9100']
        labels:
          node: "local-mac"
          provider: "local"

      # Lambda GH200 (node_exporter on port 9100)
      - targets: ['100.71.89.91:9100']
        labels:
          node: "lambda-gh200-1"
          provider: "lambda"
      - targets: ['100.110.143.119:9100']
        labels:
          node: "lambda-gh200-2"
          provider: "lambda"
      - targets: ['100.69.101.108:9100']
        labels:
          node: "lambda-gh200-3"
          provider: "lambda"
      - targets: ['100.77.186.124:9100']
        labels:
          node: "lambda-gh200-4"
          provider: "lambda"
      - targets: ['100.83.177.16:9100']
        labels:
          node: "lambda-gh200-5"
          provider: "lambda"
      - targets: ['100.68.208.43:9100']
        labels:
          node: "lambda-gh200-training"
          provider: "lambda"
      - targets: ['100.121.230.110:9100']
        labels:
          node: "lambda-gh200-8"
          provider: "lambda"
      - targets: ['100.127.168.116:9100']
        labels:
          node: "lambda-gh200-9"
          provider: "lambda"
      - targets: ['100.100.19.96:9100']
        labels:
          node: "lambda-gh200-10"
          provider: "lambda"
      - targets: ['100.106.87.89:9100']
        labels:
          node: "lambda-gh200-11"
          provider: "lambda"

      # Nebius
      - targets: ['100.110.28.41:9100']
        labels:
          node: "nebius-backbone-1"
          provider: "nebius"
      - targets: ['100.106.19.6:9100']
        labels:
          node: "nebius-h100-1"
          provider: "nebius"
      - targets: ['100.109.195.71:9100']
        labels:
          node: "nebius-h100-3"
          provider: "nebius"

      # Vultr
      - targets: ['100.94.201.92:9100']
        labels:
          node: "vultr-a100-20gb"
          provider: "vultr"

      # Hetzner
      - targets: ['100.94.174.19:9100']
        labels:
          node: "hetzner-cpu1"
          provider: "hetzner"
      - targets: ['100.67.131.72:9100']
        labels:
          node: "hetzner-cpu2"
          provider: "hetzner"
      - targets: ['100.126.21.102:9100']
        labels:
          node: "hetzner-cpu3"
          provider: "hetzner"

  # ==========================================================================
  # Leader-only services (running on coordinator)
  # ==========================================================================
  - job_name: 'ringrift-unified-loop'
    static_configs:
      - targets: ['localhost:9091']
        labels:
          instance: 'leader'
    scrape_interval: 15s

  - job_name: 'ringrift-elo-metrics'
    static_configs:
      - targets: ['localhost:9092']
        labels:
          instance: 'leader'
    scrape_interval: 30s

  - job_name: 'ringrift-data-quality'
    static_configs:
      - targets: ['localhost:9093']
        labels:
          instance: 'leader'
    scrape_interval: 60s

  # ==========================================================================
  # Prometheus Federation - aggregate metrics from remote Prometheus instances
  # Useful when nodes run their own Prometheus for local dashboards
  # ==========================================================================
  - job_name: 'prometheus-federation'
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job=~"ringrift.*"}'
        - '{job="node_exporter"}'
        - '{__name__=~".*_total|.*_seconds|.*_bytes"}'
    static_configs:
      - targets: ['100.68.208.43:9090']
        labels:
          source_node: "lambda-gh200-training"
      - targets: ['100.100.19.96:9090']
        labels:
          source_node: "lambda-gh200-10"
    scrape_interval: 60s
    scrape_timeout: 30s
